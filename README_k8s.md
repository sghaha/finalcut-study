## 1. 핵심 컨셉
- 클러스터 아키텍처
- API primitives
- Services & Other Network Primitives


## 2. k8s의 목적
- 자동화된 방식으로 우리의 응용프로그램을 컨테이너 방식으로 호스팅

## 3. 워커노드
- 컨테이너를 로딩할 수 있는 배라고 생각하면됨

## 4. 마스터 노드(컨트롤 플레인)
- 워커노드를 관리하고 컨트롤함
- 클러스터를 관리
- 노드에 대한 정보를 저장
- 어떤 컨테이너가 어디에 있을지 계획
- 노드와 컨테이너를 모니터링
- 등등

## 5. ETCD cluster
- 어디에 어떤 컨테이너가 있고, 언제부터 적재되었고, 같은 정보를 키-밸류로 저장
- "앳,씨티" 정도로 읽음

## 6. kube-scheduler

## 7. Controller-Manager (Node-Controller / Replication-Controller)
- 컨테이너가 손상되거나 파손되면 새 컨테이너를 준비해주거나 함
### 7.1. Node-Controller
- 노드를 관리, 사용 불가능하거나 파괴되는 상황을 처리
### 7.2 Replication-Controller
- 원하는 컨테이너 수가 Replication 그룹에서 항상 실행되도록

## 8. kube-apiserver
- k8s 주요 관리 구성요소
- 클러스터내에서 모든 작업을 오케스트레이션함
- 주기적으로 kubelet으로부터 상태 정보를 가져옴 (컨테이너를 모니터링 하기 위해)

## 9. 컨테이너를 적재하기 위해 모든 노드에는 도커가 설치되어있음
- 사실 무조건 도커일 필요는 없긴하다
  - 여담 : v1.24 부터 dockershim지원을 종료했다는데 궁금하면 나중에 더 알아보자

## 10. kubelet (큐블렛)
- 모든 노드에는 선장이 있다.
- 선장은 각 노드와 통신을 맡는다.
- 클러스터의 각 노드에서 실행되는 에이전트
- kube api 서버의 지시를 듣고 행동을 취함(컨테이너를 배포하거나 파괴)

## 11. kube-proxy
- 하나에 노드에 was가 있고 다른 노드에 db가 있다고 생각해보자 어떻게 통신할까? 이걸 해주는게 kube-proxy service
- 워커 노드에 필요한 룰이 실행되도록 합니다.
- 서로가 통신할 수 있도록


## 12. 마스터 노드에 있는것
- etcd클러스터 : 클러스터에 관한 정보를 저장
- kube-scheduler : 노드의 응용프로그램이나 컨테이너의 스케즁을 짜는 역할
- kube-apiserver : 서버내에서 모든 작업을 오케스트레이션 함
- controller-manager

## 13. 워커노드에 있는것
- kubelet : kube api 서버의 지시를 듣고 컨테이너와 kube-proxy를 관리
- kube-proxy : 클러스터 내부의 서비스간 통신을 가능캐함
- 컨테이너 엔진(도커같은거)



---
## 14. ETCD
- 분산되고 신뢰할 수 있는 키 밸류 스토어
- "엣,씨디"정도로 발음
- 실행하면 기본적으로 2379 포트에서 동작

### 14.0. etcd가 저장하는 정보
- nodes
- PODs
- Configs
- Secrets
- Accounts
- Roles
- Bindings
- Others

### 14.1. key-value store
- key-value 장점 : 
  1) 컬럼을 하나 만들때 모든 row에 대해 적용하지 않아도 된다.
  2) 다른 문서를 업데이트 하지 않고 추가적인 세부사항을 넣을 수 있다.
  
### 14.2. ETCD Controller client
* 저장하려면
```
./etcdctl set key1 value1
```
* v3에서는
```
./etcdctl put key1 value1
```
* 가져오려면
```
./etcdctl get key1 
```


---
## 15. kube-apiserver
* pod를 생성한다고 생각하자
  1) 사용자가 kubectl 명령을 내리면
  2) kube-apiserver가 그걸 받아서 인증 처리하고
  3) 요청을 검증하고
  4) 기존 데이터를 etcd에서 확인하고
  5) etcd를 업데이트 하고
  6) kube-schduler는 api서버를 지속적으로 관찰하고 노드에 할당되지 않은파드가 있다는걸 알아치리고
  7) 이때 스케쥴러가 올바른 노드에 새 파드를 넣어주고 api서버와 통신을함
  8) 이제 api서버가 kubelet에게 알려주고
  9) 큐블렛은 파드를 생성하고 컨테이너 런타임 엔젠에 지시해서 배포함
  10) 완료되면 큐블렛은 상태를 api서버로 보냄
  11) api서버는 다시 etcd데이터를 업데이트 함

- 클러스터에서 변경을 위해 수행해야하는 모든 작업의 중심에 있다.
- 요청의 인증과 유효성을 확인하고 데이터 스토어에서 데이터를 검색하고 업데이트함
- kube-apiserver는 사실 저장소(etcd)와 상호작용하는 유일한 구성요소임

---
## 16. kube controller manager
관제소 같은 느낌임
- 상태를 살피고
- 상황을 조정하기 위해 조치를 취함
- k8s에는 아래 두개의 컨트롤러 말고도 컨트롤러가 많음 (디플로이먼트 컨트롤러, 네임스페이스 컨트롤러, 엔드포인트 컨트롤러, 잡 컨트롤러 등등)
- 이 많은 컨트롤러들이 kube controller manager로 패키지화 되어있다.

### 16.1 Node Controller
- Node의 상태를 모니터링하고
- 5초마다 kube api server에 노드 상태를 물어본다 (Node Monitor period = 5s)
- 노드의 신호가 안잡히는건 40초 주기로 알 수 있음(Node Monitor Grace period = 40s)
- 파드가 다시 뜰 때까지는 5분이 걸린다. (POD Eviction Timeout = 5m)

### 16.2 Replication Controller

- Replication Set의 상태를 모니터링하고 원하는 수의 파드가 세트내에서 항상 떠있도록함
- 파드가 죽으면 다른 파드가 생김

---
## 17. Kube Scheduler
- 노드 파드의 일정 관리를 책임짐
- 스케쥴러는 어떤 파드가 어떤 노드에 들어갈지만 결정함
- 파드를 노드에 두는건 큐블렛이 할일임, 스케쥴러는 결정만함 

### 17.1 왜 스케쥴러가 필요한가?
- 알맞은 컨테이너를 알맞은 배에 실어야함
- 컨테이너와 배 크기가 다를수 있음 각각에 알맞게 배정해줘야함
- 각 배의 목적지가 다를수도있음
- k8s는 스케쥴러가 특정기준에 따라 포드를 어느 노드에 놓을지 정함

### 17.1 어떤 노드에 파드를 둘지 결정하는걸 자세히 알아보자
- 파드를 놓을 최고의 노드를 찾으려한다.
- 각파드에는 요구하는 cpu와 RAM등의 요구 사항이 있다.
- 스케쥴러는 두단계를 거쳐 적합한 노드를 찾는다.
  1) 이 파드에 맞지 않는 노드를 걸러낸다.
  2) 남은 노드중 점수를 메긴다. 더 자원이 여유로운 걸 고른다.
- 적합한 노드를 찾을때 점수먹이는건 내가 조절할 수 있다.

### 17.2 스케쥴러 옵션을 불수 있는 파일
```
cat /etc/kubernetes/manifests/kube-scheduler.yaml
```



--- 

## 18. kubelet
- 배의 선장같은 느낌
- 마스터십과의 유일한 연락책
- 배에 컨테이너를 싣거나 내림 
- 스케쥴러가 하라는 대로한다 (물론 스케쥴러는 api server를 통해 알려줌)
- 파드와 컨테이너를 모니터링 하고 동시에 kube api 서버네 보고한다.
- 큐블렛은 다른것과 다르게 무조건 수동으로 설치해야하는 거라고한다.




---
## 19. Kube Proxy
- 파드 네트워크는 내부 가상 네트워크로 모든 포드가 연결되는 클러스터내 모든 노드에 걸쳐있다.
- 이걸 통신하게 해줌
- was파드랑 db파드가 있다 was가 db에 연결 할때 ip를 통해 연결하면 되지만 항상 이 ip가 같을 거라는 보장이 없다.
- 이걸 위해 클러스터에 걸쳐 DB를 노출할 서비스를 생성한다.
- 이제 그 서비스를 이용해서 was가 db에 접근한다.
- 서비스는 거기에 할당된 ip주소도 받는다.

### 19.1 그럼 서비스는 뭐고 ip는 어떻게 얻음?
- 서비스는 pod 네트워크에 조인할 수 없다. 
- 사실 서비스는 실제로 존재하는것이 아니기 때문에(파드 같은 컨테이너가 아니라서)
- k8s 메모리에만 존재하는 가상 구성요소임
- 그럼 어캐 파드들이 서비스에 접근 가능한거임??
- 그래서!!!! 큐브 프록시가 필요함

### 19.1 큐브 프록시 좀더 설명
- k8s 클러스터의 각 노드에서 실행되는 프로세스
- 새 서비스를 찾아줌
- 새 서비스가 생성될 때마다 각 노드에 적절한 규칙을 만들어 그 서비스로 트래픽을 전달함

### 19.2 이걸하기 위한 방법
- iptables규칙을 이용함 



---
## 20. Pods
- 파드는 앱의 단일 인스턴스
- 파드는 k8s에서 만들수 있는 가장 작은 단위
- 만약에 트래픽이 증가해서 스케일아웃을 할떄 새로운 파드를 하나더 띄우면됨
- 더 증가하면?? 근데 노드에 파드가 꽉찼다면?? 노드를 하나 더 띄워서 파드를 더 띄우면됨
- 보통 파드는 앱을 실행하는 컨테이너와 1:1 관계임
- 그런데 같은 파드안에 떠있는 컨테이너를 도와주는 컨테이너가 있을수 있다. 
- 사용자가 업로드한 파일처리 같은경우가 그 경우임, 이경우에는 컨테이너 두개를 같은 포드에 둘수 있다. (이경우는 두 컨테이너는 하나의 로컬호스트에 있다고 생각하면됨)
- 근데 이렇게 하나의 파드에 여러개 컨테이너 띄우는건 드문경우다

### 20.1 pod 배포 방법 예시
1) 파드 띄우기 run
```
kubectl run nginx --image nginx
```
* Docker hub 레포지토리에서 nginx 이미지를 다운로드 받아서 띄운다.

2) pod 목록보기
```
kubectl get pods
```

3) 여기까지 하면
* 외부에서 nginx접근은 하지 못한다. 하지만 내부적으로 액세스는 가능하다.



### 20.2 with yaml
* 크게 아래 4개의 것들을 필드로 가지고있다.
```
apiVersion:
kind:
metadata:


spec:
```

- apiVersion : k8s버전, 우린 파드 생성을 해볼거니까 일단 v1으로 하자
  1) pod : v1
  2) Service : v1
  3) ReplicaSet : apps/v1
  4) Deployment : apps/v1
- kind : 만들려는 유형, 여기서는 Pod
- metadata : 네임이나 레이블 같은거 
- spec : 스펙정보 저장,

   
- 만들어진 yaml vkdlf
```
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:  #이건 list혹은 Array임 그래서 바로 밑에 - 이게 있는거임
  - name: nginx-container
    image: nginx
```

### 20.3 yaml파일로 pod를 만드는법
```
kubectl create -f pod-definitaion.yml
```
* apply도 사용가능























  


